"""HTML Reporter for iOS Test Optimizer.

Generates styled HTML reports from analyzer results.
"""

from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

from xcuitest_goblin.config import get_threshold


class HTMLReporter:
    """Generates HTML analysis reports from analyzer results."""

    def __init__(self, project_path: Path):
        self.project_path = Path(project_path)

    def generate_report(
        self,
        test_inventory: Optional[Dict[str, Any]] = None,
        accessibility_data: Optional[Dict[str, Any]] = None,
        test_plans: Optional[Dict[str, Any]] = None,
        screen_graph: Optional[Dict[str, Any]] = None,
    ) -> str:
        timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")

        sections = {
            "executive_summary": self._build_executive_summary(
                test_inventory, accessibility_data, test_plans, screen_graph
            ),
            "toc": self._build_toc(
                test_inventory, accessibility_data, test_plans, screen_graph
            ),
            "test_inventory": (
                self._build_test_inventory(test_inventory) if test_inventory else ""
            ),
            "accessibility": (
                self._build_accessibility(accessibility_data)
                if accessibility_data
                else ""
            ),
            "test_plans": (self._build_test_plans(test_plans) if test_plans else ""),
            "screen_graph": (
                self._build_screen_graph(screen_graph)
                if screen_graph and screen_graph.get("has_screen_graph")
                else ""
            ),
            "recommendations": self._build_recommendations(
                test_inventory, accessibility_data, test_plans, screen_graph
            ),
        }

        return self._render_html(timestamp, sections)

    def _render_html(self, timestamp: str, sections: Dict[str, str]) -> str:
        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>iOS Test Suite Analysis Report</title>
    <style>{self._get_css()}</style>
</head>
<body>
    <div class="container">
        <header>
            <h1>iOS Test Suite Analysis Report</h1>
            <p class="meta">
                Generated: {timestamp}<br>
                Project: <code>{self.project_path.name}</code>
            </p>
        </header>
        {sections['executive_summary']}
        {sections['toc']}
        {sections['test_inventory']}
        {sections['accessibility']}
        {sections['test_plans']}
        {sections['screen_graph']}
        {sections['recommendations']}
        <footer>Generated by <strong>iOS Test Optimizer</strong></footer>
    </div>
</body>
</html>"""

    def _get_css(self) -> str:
        return """
        :root {
            --primary: #007AFF;
            --success: #34C759;
            --warning: #FF9500;
            --danger: #FF3B30;
            --bg: #f5f5f7;
            --card-bg: #ffffff;
            --text: #1d1d1f;
            --text-secondary: #86868b;
            --border: #d2d2d7;
        }
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Text',
                         'Segoe UI', Roboto, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.5;
            padding: 2rem;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        header {
            text-align: center;
            margin-bottom: 2rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border);
        }
        h1 { font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem; }
        .meta { color: var(--text-secondary); font-size: 0.9rem; }
        .meta code {
            background: var(--bg);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-size: 0.85rem;
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin-bottom: 1rem;
        }
        .summary-grid.issue-row { margin-bottom: 0.75rem; }
        .summary-grid.info-row {
            margin-bottom: 2rem;
        }
        .summary-grid.info-row .summary-card {
            padding: 1rem;
        }
        .summary-grid.info-row .value {
            font-size: 1.5rem;
        }
        .summary-card {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 1.5rem;
            text-align: center;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        .summary-card.issue-card {
            border-left: 4px solid var(--border);
        }
        .summary-card.issue-card.danger {
            border-left-color: var(--danger);
        }
        .summary-card.issue-card.danger .value { color: var(--danger); }
        .summary-card.issue-card.warning {
            border-left-color: var(--warning);
        }
        .summary-card.issue-card.warning .value { color: var(--warning); }
        .summary-card.issue-card.success {
            border-left-color: var(--success);
        }
        .summary-card.issue-card.success .value { color: var(--success); }
        .summary-card .value {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--primary);
        }
        .summary-card .label {
            color: var(--text-secondary);
            font-size: 0.9rem;
            margin-top: 0.25rem;
        }
        section {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 1.5rem 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        h2 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--primary);
        }
        h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 0.75rem 0;
            padding-top: 1.5rem;
            color: var(--text);
        }
        h3:first-of-type {
            margin-top: 0;
            padding-top: 0;
        }
        .section-divider {
            width: 80px;
            height: 1px;
            background: var(--border);
            margin: 2rem auto;
        }
        h4 {
            margin-top: 1.5rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            font-size: 0.9rem;
        }
        th, td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }
        th {
            background: var(--bg);
            font-weight: 600;
            color: var(--text-secondary);
            text-transform: uppercase;
            font-size: 0.75rem;
            letter-spacing: 0.5px;
        }
        tr:hover { background: var(--bg); }
        .stat-row {
            display: flex;
            justify-content: space-between;
            padding: 0.5rem 0;
            border-bottom: 1px solid var(--border);
        }
        .stat-row:last-child { border-bottom: none; }
        .stat-label { color: var(--text-secondary); }
        .stat-value { font-weight: 600; }
        .badge {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            border-radius: 999px;
            font-size: 0.8rem;
            font-weight: 500;
        }
        .badge-success { background: #d1f7dd; color: #1a7f37; }
        .badge-warning { background: #fff3cd; color: #856404; }
        .badge-danger { background: #fee2e2; color: #991b1b; }
        .badge-info { background: #dbeafe; color: #1e40af; }
        .recommendation {
            padding: 1rem 1.25rem;
            margin: 0.75rem 0;
            border-left: 4px solid var(--warning);
            background: #fffbeb;
            border-radius: 0 8px 8px 0;
        }
        .recommendation.critical {
            border-left-color: var(--danger);
            background: #fef2f2;
        }
        .recommendation h4 {
            font-size: 1rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
        }
        .recommendation ul {
            margin: 0.5rem 0 0 1.5rem;
            color: var(--text-secondary);
        }
        .recommendation li { margin: 0.25rem 0; }
        .recommendation code {
            background: rgba(0,0,0,0.05);
            padding: 0.1rem 0.4rem;
            border-radius: 3px;
            font-size: 0.85rem;
        }
        .file-list {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 0.5rem;
        }
        .file-tag {
            background: var(--bg);
            padding: 0.25rem 0.75rem;
            border-radius: 6px;
            font-size: 0.85rem;
            font-family: 'SF Mono', Monaco, monospace;
        }
        .json-ref {
            color: var(--text-secondary);
            font-size: 0.85rem;
            margin-top: 0.75rem;
        }
        .json-ref a {
            background: var(--bg);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            color: var(--primary);
            text-decoration: none;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 0.85rem;
        }
        .json-ref a:hover {
            text-decoration: underline;
            background: #e8e8ed;
        }
        .progress-bar {
            height: 8px;
            background: var(--border);
            border-radius: 4px;
            overflow: hidden;
            margin: 0.5rem 0;
        }
        .progress-fill {
            height: 100%;
            border-radius: 4px;
            transition: width 0.3s;
        }
        .progress-fill.good { background: var(--success); }
        .progress-fill.warning { background: var(--warning); }
        .progress-fill.danger { background: var(--danger); }
        .naming-category {
            margin: 1rem 0;
            padding: 1rem;
            background: var(--bg);
            border-radius: 8px;
        }
        .naming-category h4 {
            font-size: 0.9rem;
            color: var(--text-secondary);
            margin-bottom: 0.5rem;
        }
        .toc {
            background: var(--card-bg);
            border-radius: 12px;
            padding: 1.5rem 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        .toc h2 {
            font-size: 1.2rem;
            margin-bottom: 1rem;
            border-bottom: none;
            padding-bottom: 0;
        }
        .toc ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem 1.5rem;
        }
        .toc a {
            color: var(--primary);
            text-decoration: none;
            font-weight: 500;
        }
        .toc a:hover { text-decoration: underline; }
        footer {
            text-align: center;
            padding: 2rem;
            color: var(--text-secondary);
            font-size: 0.85rem;
        }
        .note {
            font-style: italic;
            color: var(--text-secondary);
            font-size: 0.9rem;
            margin-top: 0.5rem;
        }
        """

    def _build_executive_summary(
        self,
        test_inventory: Optional[Dict[str, Any]],
        accessibility_data: Optional[Dict[str, Any]],
        test_plans: Optional[Dict[str, Any]],
        screen_graph: Optional[Dict[str, Any]],
    ) -> str:
        # Calculate issue counts for top row (actionable metrics)
        naming_issues = None  # None means not configured
        orphaned_tests = 0
        large_files = 0

        if test_inventory:
            # Calculate naming issues from convention stats (if configured)
            naming = test_inventory.get("naming_patterns")
            if naming:
                total_files = test_inventory.get("total_test_files", 0)
                follows_convention = naming.get("follows_convention", total_files)
                naming_issues = total_files - follows_convention

            # Count large files (exceeding threshold)
            threshold = get_threshold("test_inventory", "large_file_threshold", 30)
            for f in test_inventory.get("test_files", []):
                if f.get("test_count", 0) > threshold:
                    large_files += 1

        if test_plans:
            orphaned_tests = test_plans.get("orphaned_count", 0)

        # Build issue cards (top row - highlighted)
        issue_cards = []
        if naming_issues is not None:
            issue_cards.append(
                self._issue_card(
                    naming_issues,
                    "Naming Issues",
                    (
                        "danger"
                        if naming_issues > 10
                        else ("warning" if naming_issues > 0 else "success")
                    ),
                )
            )
        issue_cards.append(
            self._issue_card(
                orphaned_tests,
                "Orphaned Tests",
                (
                    "danger"
                    if orphaned_tests > 50
                    else ("warning" if orphaned_tests > 0 else "success")
                ),
            )
        )
        issue_cards.append(
            self._issue_card(
                large_files,
                "Large Files",
                (
                    "danger"
                    if large_files > 5
                    else ("warning" if large_files > 0 else "success")
                ),
            )
        )

        # Build info cards (bottom row - informational)
        info_cards = []
        if test_inventory:
            info_cards.append(
                self._info_card(test_inventory.get("total_test_methods", 0), "Tests")
            )
            info_cards.append(
                self._info_card(test_inventory.get("total_test_files", 0), "Test Files")
            )
        if test_plans:
            info_cards.append(
                self._info_card(len(test_plans.get("test_plans", [])), "Test Plans")
            )

        return f"""<div class="summary-grid issue-row">{"".join(issue_cards)}</div>
        <div class="summary-grid info-row">{"".join(info_cards)}</div>"""

    def _issue_card(self, value: Any, label: str, severity: str = "warning") -> str:
        return f"""
        <div class="summary-card issue-card {severity}">
            <div class="value">{value}</div>
            <div class="label">{label}</div>
        </div>"""

    def _info_card(self, value: Any, label: str) -> str:
        return f"""
        <div class="summary-card info-card">
            <div class="value">{value}</div>
            <div class="label">{label}</div>
        </div>"""

    def _format_strategy(self, strategy: str) -> str:
        """Convert technical strategy names to user-friendly labels."""
        strategy_labels = {
            "negative_selection": "Include All",
            "positive_selection": "Explicit Only",
            "randomized": "Randomized",
        }
        return strategy_labels.get(strategy, strategy.replace("_", " ").title())

    def _build_toc(
        self,
        test_inventory: Optional[Dict[str, Any]],
        accessibility_data: Optional[Dict[str, Any]],
        test_plans: Optional[Dict[str, Any]],
        screen_graph: Optional[Dict[str, Any]],
    ) -> str:
        items = []
        if test_inventory:
            items.append('<li><a href="#test-inventory">Test Inventory</a></li>')
        if accessibility_data:
            items.append('<li><a href="#accessibility">Accessibility IDs</a></li>')
        if test_plans:
            items.append('<li><a href="#test-plans">Test Plans</a></li>')
        if screen_graph and screen_graph.get("has_screen_graph"):
            items.append('<li><a href="#screen-graph">Screen Graph</a></li>')
        items.append('<li><a href="#recommendations">Recommendations</a></li>')
        return f"""
        <nav class="toc">
            <h2>Contents</h2>
            <ul>{"".join(items)}</ul>
        </nav>"""

    def _build_test_inventory(self, data: Dict[str, Any]) -> str:
        total_files = data.get("total_test_files", 0)
        total_methods = data.get("total_test_methods", 0)

        # Get stats from tests_per_file
        stats = data.get("tests_per_file", {})
        min_t = stats.get("min", 0)
        max_t = stats.get("max", 0)
        avg_t = stats.get("avg", 0)
        median_t = stats.get("median", 0)

        # Get naming patterns (may be None if not configured)
        naming = data.get("naming_patterns")
        naming_html = ""
        if naming:
            pattern = naming.get("pattern", "")
            consistency_str = naming.get("consistency", "0%")
            consistency = float(consistency_str.replace("%", ""))

            progress_class = (
                "good"
                if consistency >= 90
                else ("warning" if consistency >= 70 else "danger")
            )

            non_compliant_html = self._build_non_compliant_files(data)

            naming_html = f"""
            <div class="section-divider"></div>
            <h3>Test File Naming</h3>
            <div class="stat-row">
                <span class="stat-label">Expected Pattern</span>
                <span class="stat-value"><code>{pattern}</code></span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Consistency</span>
                <span class="stat-value">{consistency:.1f}%</span>
            </div>
            <div class="progress-bar">
                <div class="progress-fill {progress_class}"
                     style="width: {consistency}%"></div>
            </div>
            {non_compliant_html}"""

        # Build method naming section (if configured)
        method_naming = data.get("method_naming_patterns")
        method_naming_html = ""
        if method_naming:
            m_style = method_naming.get("expected_style", "camelCase")
            m_consistency_str = method_naming.get("consistency", "0%")
            m_consistency = float(m_consistency_str.replace("%", ""))
            m_breakdown = method_naming.get("style_breakdown", {})
            m_non_compliant = method_naming.get("non_compliant_methods", [])
            m_non_compliant_count = method_naming.get(
                "non_compliant_count", len(m_non_compliant)
            )

            m_progress_class = (
                "good"
                if m_consistency >= 85
                else ("warning" if m_consistency >= 70 else "danger")
            )

            # Build breakdown display
            breakdown_items = " | ".join(
                [
                    f"{style}: {count}"
                    for style, count in m_breakdown.items()
                    if count > 0
                ]
            )

            # Build non-compliant methods table
            m_non_compliant_html = ""
            if m_non_compliant and m_consistency < 100:
                # Build table rows (show first 20)
                method_rows = ""
                for item in m_non_compliant[:20]:
                    method = (
                        item.get("method", item) if isinstance(item, dict) else item
                    )
                    style = (
                        item.get("detected_style", "unknown")
                        if isinstance(item, dict)
                        else "unknown"
                    )
                    method_rows += f"""<tr>
                        <td><code>{method}</code></td>
                        <td>{style}</td>
                    </tr>"""

                note = ""
                if m_non_compliant_count > 20:
                    note = f'<p class="note">Showing 20 of {m_non_compliant_count} non-compliant methods</p>'

                m_non_compliant_html = f"""
                <h4>Methods Not Following Convention
                    <span class="badge badge-warning">{m_non_compliant_count}</span>
                </h4>
                <table>
                    <thead>
                        <tr><th>Method</th><th>Detected Style</th></tr>
                    </thead>
                    <tbody>{method_rows}</tbody>
                </table>
                {note}
                <p class="json-ref">See <a href="test_inventory.json">test_inventory.json</a> →
                   <code>method_naming_patterns.non_compliant_methods</code> for complete list</p>"""

            method_naming_html = f"""
            <div class="section-divider"></div>
            <h3>Test Method Naming</h3>
            <div class="stat-row">
                <span class="stat-label">Expected Style</span>
                <span class="stat-value"><code>{m_style}</code></span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Consistency</span>
                <span class="stat-value">{m_consistency:.1f}%</span>
            </div>
            <div class="progress-bar">
                <div class="progress-fill {m_progress_class}"
                     style="width: {m_consistency}%"></div>
            </div>
            <p class="note">Style breakdown: {breakdown_items}</p>
            {m_non_compliant_html}"""

        # Build largest files table (top 20)
        files = data.get("test_files", [])
        sorted_files = sorted(
            files, key=lambda x: x.get("test_count", 0), reverse=True
        )[:20]
        files_rows = "".join([f"""<tr>
                <td><code>{f.get('file_name', '')}</code></td>
                <td>{f.get('test_count', 0)}</td>
                <td>{', '.join(f.get('test_classes', []))}</td>
            </tr>""" for f in sorted_files])

        return f"""
        <section id="test-inventory">
            <h2>Test Inventory</h2>
            <h3>Overview</h3>
            <div class="stat-row">
                <span class="stat-label">Total Test Files</span>
                <span class="stat-value">{total_files}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Total Test Methods</span>
                <span class="stat-value">{total_methods}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Tests per File</span>
                <span class="stat-value">
                    {min_t} min, {max_t} max, {avg_t:.1f} avg, {median_t} median
                </span>
            </div>
            <p class="json-ref">Complete data: <a href="test_inventory.json">test_inventory.json</a></p>
            {naming_html}
            {method_naming_html}

            <div class="section-divider"></div>
            <h3>Largest Test Files</h3>
            <table>
                <thead>
                    <tr><th>File</th><th>Tests</th><th>Classes</th></tr>
                </thead>
                <tbody>{files_rows}</tbody>
            </table>
            <p class="note">Showing top 20 of {total_files} files</p>
            <p class="json-ref">See <a href="test_inventory.json">test_inventory.json</a> →
               <code>test_files</code> for complete list</p>
        </section>"""

    def _build_non_compliant_files(self, data: Dict[str, Any]) -> str:
        files = data.get("test_files", [])
        naming = data.get("naming_patterns")
        if not naming:
            return ""

        follows = naming.get("follows_convention", 0)
        total = len(files)
        non_compliant_count = total - follows

        if non_compliant_count == 0:
            return ""

        # Categorize non-compliant files
        categories: Dict[str, List[str]] = {
            "Snake case": [],
            "Flow files": [],
            "Validation files": [],
            "Scenario files": [],
            "Other": [],
        }

        for f in files:
            name = f.get("file_name", "")
            # Check if it follows convention
            if name.endswith("Tests.swift") and "_" not in name:
                continue  # Compliant
            if name.endswith("Test.swift") and "_" not in name:
                continue  # Also acceptable

            # Categorize
            if "_" in name:
                categories["Snake case"].append(name)
            elif "Flow" in name and not name.endswith("Tests.swift"):
                categories["Flow files"].append(name)
            elif "Validation" in name and not name.endswith("Tests.swift"):
                categories["Validation files"].append(name)
            elif "Scenario" in name and not name.endswith("Tests.swift"):
                categories["Scenario files"].append(name)
            elif not name.endswith("Tests.swift") and not name.endswith("Test.swift"):
                categories["Other"].append(name)

        html_parts = [
            f"<p><strong>Files Not Following Convention ({non_compliant_count}):</strong></p>"
        ]

        for cat, cat_files in categories.items():
            if not cat_files:
                continue
            file_tags = " ".join(
                [f'<span class="file-tag">{f}</span>' for f in cat_files[:5]]
            )
            more = (
                f' <span class="note">...and {len(cat_files) - 5} more</span>'
                if len(cat_files) > 5
                else ""
            )
            html_parts.append(f"""
            <div class="naming-category">
                <h4>{cat} ({len(cat_files)} files)</h4>
                <div class="file-list">{file_tags}{more}</div>
            </div>""")

        html_parts.append(
            '<p class="json-ref">See <a href="test_inventory.json">test_inventory.json</a> → '
            "<code>test_files</code> for complete list</p>"
        )
        return "".join(html_parts)

    def _build_accessibility(self, data: Dict[str, Any]) -> str:
        total_ids = data.get("total_unique_ids", 0)
        total_usage = data.get("total_usage_count", 0)
        avg_usage = total_usage / total_ids if total_ids > 0 else 0

        conventions = data.get("naming_conventions", {})
        pascal = conventions.get("PascalCase", 0)
        dotted = conventions.get("dotted_notation", 0)
        lower = conventions.get("lowercase", 0)

        # Categorize identifiers by naming convention for samples
        all_identifiers = data.get("identifiers", [])
        pascal_samples = []
        dotted_samples = []
        lower_samples = []
        for item in all_identifiers:
            id_val = item.get("id", "")
            if "." in id_val:
                dotted_samples.append(id_val)
            elif id_val and id_val[0].isupper():
                pascal_samples.append(id_val)
            elif id_val.islower():
                lower_samples.append(id_val)

        # Build top 20 identifiers table
        top_ids = data.get("top_20_most_used", [])[:20]
        ids_rows = ""
        for i, item in enumerate(top_ids, 1):
            count = item.get("usage_count", 0)
            badge = ""
            if count > 100:
                badge = '<span class="badge badge-danger">overused</span>'
            elif count > 50:
                badge = '<span class="badge badge-warning">high usage</span>'
            ids_rows += f"""
            <tr>
                <td>{i}</td>
                <td><code>{item.get('id', '')}</code> {badge}</td>
                <td>{count}</td>
            </tr>"""

        # Generic IDs warning
        generic_warning = ""
        generic_ids = [i for i in top_ids if i.get("usage_count", 0) > 50]
        if generic_ids:
            items = ", ".join(
                [
                    f"<code>{i.get('id', '')}</code> ({i.get('usage_count', 0)} uses)"
                    for i in generic_ids[:5]
                ]
            )
            generic_warning = f"""
            <h3>Warning: Potentially Generic IDs</h3>
            <p>The following IDs are used very frequently and may be too generic:</p>
            <p>{items}</p>"""

        # Unused identifiers section
        unused_ids = data.get("unused_identifiers", [])
        unused_count = data.get("unused_count", len(unused_ids))
        unused_html = ""
        if unused_count > 0:
            # Get full identifier data for defined_in info
            all_identifiers = data.get("identifiers", [])
            unused_data = [i for i in all_identifiers if i.get("id") in unused_ids]

            # Build table rows (show first 20)
            unused_rows = ""
            for item in unused_data[:20]:
                defined_in = item.get("defined_in", [])
                location = ", ".join(defined_in) if defined_in else "Unknown"
                unused_rows += f"""<tr>
                    <td><code>{item.get('id', '')}</code></td>
                    <td>{location}</td>
                </tr>"""

            note = ""
            if unused_count > 20:
                note = f'<p class="note">Showing 20 of {unused_count} unused identifiers</p>'

            unused_html = f"""
            <div class="section-divider"></div>
            <h3>Unused Identifiers
                <span class="badge badge-warning">{unused_count}</span>
            </h3>
            <p>The following identifiers are defined in the codebase but never used in tests:</p>
            <table>
                <thead>
                    <tr><th>Identifier</th><th>Defined In</th></tr>
                </thead>
                <tbody>{unused_rows}</tbody>
            </table>
            {note}
            <p class="json-ref">See <a href="accessibility_ids.json">accessibility_ids.json</a> →
               <code>unused_identifiers</code> for complete list</p>"""

        return f"""
        <section id="accessibility">
            <h2>Accessibility Identifiers</h2>
            <h3>Overview</h3>
            <div class="stat-row">
                <span class="stat-label">Total Unique IDs</span>
                <span class="stat-value">{total_ids}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Total Usage Count</span>
                <span class="stat-value">{total_usage}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Average Usage per ID</span>
                <span class="stat-value">{avg_usage:.1f}</span>
            </div>
            <p class="json-ref">Complete data: <a href="accessibility_ids.json">accessibility_ids.json</a></p>

            <div class="section-divider"></div>
            <h3>Naming Conventions</h3>
            <table>
                <thead>
                    <tr><th>Convention</th><th>Count</th><th>Examples</th></tr>
                </thead>
                <tbody>
                    <tr>
                        <td>PascalCase</td>
                        <td>{pascal}</td>
                        <td><code>{"</code>, <code>".join(pascal_samples[:3]) if pascal_samples else "—"}</code></td>
                    </tr>
                    <tr>
                        <td>dotted.notation</td>
                        <td>{dotted}</td>
                        <td><code>{"</code>, <code>".join(dotted_samples[:3]) if dotted_samples else "—"}</code></td>
                    </tr>
                    <tr>
                        <td>lowercase</td>
                        <td>{lower}</td>
                        <td><code>{("</code>, <code>".join(lower_samples[:3])) if lower_samples else "—"}</code></td>
                    </tr>
                </tbody>
            </table>
            <p class="json-ref">See <a href="accessibility_ids.json">accessibility_ids.json</a> →
               <code>identifiers</code> for complete list</p>

            <div class="section-divider"></div>
            <h3>Top 20 Most Used Identifiers</h3>
            <table>
                <thead>
                    <tr><th>Rank</th><th>Identifier</th><th>Usage Count</th></tr>
                </thead>
                <tbody>{ids_rows}</tbody>
            </table>
            <p class="note">Showing top 20 of {total_ids} identifiers</p>
            <p class="json-ref">See <a href="accessibility_ids.json">accessibility_ids.json</a> →
               <code>identifiers</code> for complete list</p>
            {generic_warning}
            {unused_html}
        </section>"""

    def _build_test_plans(self, data: Dict[str, Any]) -> str:
        plans = data.get("test_plans", [])
        total_unique = data.get("total_unique_tests", 0)
        multi_plan_count = data.get("tests_in_multiple_plans_count", 0)
        orphaned_count = data.get("orphaned_count", 0)

        # Test plans table - simple list with test counts
        plans_rows = ""
        for plan in plans:
            # Calculate tests in plan: total - skipped for Include All plans
            skipped = plan.get("tests_skipped", 0)
            tests_in_plan = total_unique - skipped if total_unique > 0 else 0
            plans_rows += f"""
            <tr>
                <td><code>{plan.get('name', '')}</code></td>
                <td>{tests_in_plan}</td>
            </tr>"""

        # Tests in multiple plans
        multi_html = self._build_multi_plan_tests(data)

        # Orphaned tests
        orphaned_html = self._build_orphaned_tests(data)

        # Skipped tests
        skipped_html = self._build_skipped_tests(data)

        return f"""
        <section id="test-plans">
            <h2>Test Plans</h2>
            <h3>Overview</h3>
            <div class="stat-row">
                <span class="stat-label">Total Test Plans</span>
                <span class="stat-value">{len(plans)}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Unique Tests in Plans</span>
                <span class="stat-value">{total_unique}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Tests in Multiple Plans</span>
                <span class="stat-value">{multi_plan_count}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Orphaned Tests</span>
                <span class="stat-value">{orphaned_count}</span>
            </div>
            <p class="json-ref">Complete data: <a href="test_plans.json">test_plans.json</a></p>

            <div class="section-divider"></div>
            <h3>Test Plan Details</h3>
            <table>
                <thead>
                    <tr>
                        <th>Plan Name</th>
                        <th>Tests</th>
                    </tr>
                </thead>
                <tbody>{plans_rows}</tbody>
            </table>
            <p class="json-ref">See <a href="test_plans.json">test_plans.json</a> →
               <code>test_plans</code> for complete list</p>

            <div class="section-divider"></div>
            {multi_html}
            <div class="section-divider"></div>
            {orphaned_html}
            <div class="section-divider"></div>
            {skipped_html}
        </section>"""

    def _build_multi_plan_tests(self, data: Dict[str, Any]) -> str:
        multi_tests = data.get("tests_in_multiple_plans", [])
        multi_count = data.get("tests_in_multiple_plans_count", len(multi_tests))

        if multi_count == 0:
            return ""

        # Show sample tests with their plan counts
        sample_rows = ""
        for item in multi_tests[:20]:
            test = item.get("test", "")
            plan_count = item.get("plan_count", 0)
            plans_list = item.get("plans", [])
            plans_preview = ", ".join(plans_list[:3])
            if len(plans_list) > 3:
                plans_preview += f", +{len(plans_list) - 3} more"
            sample_rows += f"""
            <tr>
                <td><code>{test}</code></td>
                <td>{plan_count}</td>
                <td>{plans_preview}</td>
            </tr>"""

        return f"""
        <h3>Tests in Multiple Plans
            <span class="badge badge-warning">{multi_count}</span>
        </h3>
        <p>Found <strong>{multi_count} tests</strong> that appear in more than one
           test plan (useful for coverage, watch for redundancy):</p>
        <table>
            <thead>
                <tr><th>Test</th><th>Plan Count</th><th>Plans</th></tr>
            </thead>
            <tbody>{sample_rows}</tbody>
        </table>
        <p class="note">Showing 20 of {multi_count} tests.
           See <a href="test_plans.json">test_plans.json</a> →
           <code>tests_in_multiple_plans</code> for complete list.</p>"""

    def _build_orphaned_tests(self, data: Dict[str, Any]) -> str:
        orphaned = data.get("orphaned_tests", [])
        orphaned_count = data.get("orphaned_count", len(orphaned))

        if orphaned_count == 0:
            return ""

        # Group by class
        by_class: Dict[str, int] = {}
        for t in orphaned:
            cls = t.split("/")[0] if "/" in t else "Unknown"
            by_class[cls] = by_class.get(cls, 0) + 1

        sorted_classes = sorted(by_class.items(), key=lambda x: x[1], reverse=True)
        rows = "".join(
            [
                f"<tr><td>{cls}</td><td>{count}</td></tr>"
                for cls, count in sorted_classes
            ]
        )

        return f"""
        <h3>Orphaned Tests
            <span class="badge badge-danger">{orphaned_count}</span>
        </h3>
        <p>Found <strong>{orphaned_count} tests</strong> that exist in test files
           but are not included in any test plan.
           These tests will not run in CI unless added to a plan.</p>
        <p><strong>Orphaned tests by file:</strong></p>
        <table>
            <thead><tr><th>File</th><th>Orphaned Tests</th></tr></thead>
            <tbody>{rows}</tbody>
        </table>
        <p class="json-ref">Complete list: See <a href="test_plans.json">test_plans.json</a> →
           <code>orphaned_tests</code> array for all {orphaned_count}
           orphaned test methods</p>"""

    def _build_skipped_tests(self, data: Dict[str, Any]) -> str:
        skipped = data.get("skipped_tests", [])
        skipped_count = data.get("skipped_tests_count", len(skipped))

        if skipped_count == 0:
            return """
            <h3>Skipped Tests</h3>
            <p>No skipped tests found.</p>"""

        # Group by class
        by_class: Dict[str, List[str]] = {}
        for t in skipped:
            parts = t.split("/")
            cls = parts[0] if len(parts) > 0 else "Unknown"
            method = parts[1] if len(parts) > 1 else t
            by_class.setdefault(cls, []).append(method)

        # Sort by count
        sorted_classes = sorted(by_class.items(), key=lambda x: len(x[1]), reverse=True)

        # Build table rows for top 20 classes
        rows = []
        for cls, methods in sorted_classes[:20]:
            sample_methods = ", ".join(methods[:3])
            if len(methods) > 3:
                sample_methods += f", +{len(methods) - 3} more"
            rows.append(f"""<tr>
                <td><code>{cls}</code></td>
                <td>{len(methods)}</td>
                <td>{sample_methods}</td>
            </tr>""")

        remaining = len(sorted_classes) - 20
        note = ""
        if remaining > 0:
            note = f'<p class="note">Showing 20 of {len(sorted_classes)} files</p>'

        return f"""
        <h3>Skipped Tests
            <span class="badge badge-warning">{skipped_count}</span>
        </h3>
        <p>Found <strong>{skipped_count} skipped test entries</strong>
           across all test plans.</p>
        <table>
            <thead>
                <tr><th>Test Class</th><th>Skipped</th><th>Sample Tests</th></tr>
            </thead>
            <tbody>{"".join(rows)}</tbody>
        </table>
        {note}
        <p class="json-ref">See <a href="test_plans.json">test_plans.json</a> →
           <code>skipped_tests</code> for complete list</p>"""

    def _build_screen_graph(self, data: Dict[str, Any]) -> str:
        total_screens = data.get("total_screens", 0)
        adoption = data.get("navigator_adoption_rate", 0)

        progress_class = (
            "good" if adoption >= 80 else ("warning" if adoption >= 50 else "danger")
        )

        top_screens = data.get("top_screens", [])[:10]
        screens_rows = "".join(
            [
                f"<tr><td><code>{s.get('screen', '')}</code></td>"
                f"<td>{s.get('usage_count', 0)}</td></tr>"
                for s in top_screens
            ]
        )

        return f"""
        <section id="screen-graph">
            <h2>Screen Graph</h2>
            <div class="stat-row">
                <span class="stat-label">Total Screens</span>
                <span class="stat-value">{total_screens}</span>
            </div>
            <div class="stat-row">
                <span class="stat-label">Navigator Adoption</span>
                <span class="stat-value">{adoption:.1f}%</span>
            </div>
            <div class="progress-bar">
                <div class="progress-fill {progress_class}"
                     style="width: {adoption}%"></div>
            </div>
            <h3>Top Screens by Usage</h3>
            <table>
                <thead><tr><th>Screen</th><th>Usage Count</th></tr></thead>
                <tbody>{screens_rows}</tbody>
            </table>
            <p class="json-ref">Complete data: <a href="screen_graph.json">screen_graph.json</a></p>
        </section>"""

    def _build_recommendations(
        self,
        test_inventory: Optional[Dict[str, Any]],
        accessibility_data: Optional[Dict[str, Any]],
        test_plans: Optional[Dict[str, Any]],
        screen_graph: Optional[Dict[str, Any]],
    ) -> str:
        recs = []

        # Get thresholds
        large_threshold = get_threshold("test_inventory", "large_file_threshold", 30)
        naming_threshold = get_threshold(
            "test_file_naming", "consistency_threshold", 90.0
        )
        generic_threshold = get_threshold(
            "accessibility_ids", "generic_id_usage_threshold", 50
        )

        if test_inventory:
            # 1. Large files
            files = test_inventory.get("test_files", [])
            large_files = [f for f in files if f.get("test_count", 0) > large_threshold]
            if large_files:
                sorted_large = sorted(
                    large_files, key=lambda x: x.get("test_count", 0), reverse=True
                )
                items = "".join(
                    [
                        f"<li><code>{f.get('file_name', '')}</code> "
                        f"({f.get('test_count', 0)} tests)</li>"
                        for f in sorted_large
                    ]
                )
                recs.append(f"""
                <div class="recommendation">
                    <h4>Split Large Test Files</h4>
                    <p>{len(large_files)} file(s) contain more than
                       {large_threshold} tests.
                       Consider splitting for better maintainability:</p>
                    <ul>{items}</ul>
                </div>""")

            # 2. Naming consistency (only if configured)
            naming = test_inventory.get("naming_patterns")
            if naming:
                consistency_str = naming.get("consistency", "100%")
                consistency = float(consistency_str.replace("%", ""))
                pattern = naming.get("pattern", "[Feature]Tests.swift")
                if consistency < naming_threshold:
                    follows = naming.get("follows_convention", 0)
                    total = len(files)
                    non_compliant = total - follows
                    # Get sample non-compliant files
                    sample_files = [
                        f.get("file_name", "")
                        for f in files
                        if "_" in f.get("file_name", "")
                        or not f.get("file_name", "").endswith("Tests.swift")
                    ][:5]
                    items = "".join(
                        [f"<li><code>{f}</code></li>" for f in sample_files]
                    )
                    more = (
                        f"<li><em>...and {non_compliant - 5} more</em></li>"
                        if non_compliant > 5
                        else ""
                    )
                    recs.append(f"""
                <div class="recommendation">
                    <h4>Standardize File Naming</h4>
                    <p>Test file naming is only {consistency:.1f}% consistent with
                       the <code>{pattern}</code> pattern.
                       Found {non_compliant} files with inconsistent naming:</p>
                    <ul>{items}{more}</ul>
                    <p class="json-ref">See <a href="test_inventory.json">test_inventory.json</a> →
                       <code>test_files</code> for complete list</p>
                </div>""")

            # 3. Method naming consistency (only if configured)
            method_naming = test_inventory.get("method_naming_patterns")
            if method_naming:
                method_threshold = get_threshold(
                    "test_method_naming", "consistency_threshold", 85.0
                )
                m_consistency_str = method_naming.get("consistency", "100%")
                m_consistency = float(m_consistency_str.replace("%", ""))
                if m_consistency < method_threshold:
                    m_style = method_naming.get("expected_style", "camelCase")
                    m_methods = method_naming.get("non_compliant_methods", [])
                    m_count = method_naming.get("non_compliant_count", len(m_methods))
                    # Extract method names from dict or use directly if string
                    m_items = "".join(
                        [
                            f"<li><code>{m.get('method', m) if isinstance(m, dict) else m}</code></li>"
                            for m in m_methods[:5]
                        ]
                    )
                    m_more = (
                        f"<li><em>...and {m_count - 5} more</em></li>"
                        if m_count > 5
                        else ""
                    )
                    recs.append(f"""
                <div class="recommendation">
                    <h4>Standardize Method Naming</h4>
                    <p>Test method naming is only {m_consistency:.1f}% consistent with
                       the <code>{m_style}</code> style.</p>
                    <ul>{m_items}{m_more}</ul>
                    <p class="json-ref">See <a href="test_inventory.json">test_inventory.json</a> →
                       <code>method_naming_patterns.non_compliant_methods</code> for complete list</p>
                </div>""")

        if accessibility_data:
            # 4. Generic IDs (renumbered)
            top_ids = accessibility_data.get("top_20_most_used", [])
            generic_ids = [
                i for i in top_ids if i.get("usage_count", 0) > generic_threshold
            ]
            if generic_ids:
                items = "".join(
                    [
                        f"<li><code>{i.get('id', '')}</code> "
                        f"({i.get('usage_count', 0)} uses)</li>"
                        for i in generic_ids
                    ]
                )
                recs.append(f"""
                <div class="recommendation">
                    <h4>Refine Generic Accessibility IDs</h4>
                    <p>The following IDs are used very frequently
                       and may be too generic:</p>
                    <ul>{items}</ul>
                    <p>Consider using more specific, context-aware identifiers.</p>
                </div>""")

            # Unused identifiers recommendation
            unused_threshold = get_threshold(
                "accessibility_ids", "unused_ids_threshold", 0
            )
            unused_ids = accessibility_data.get("unused_identifiers", [])
            unused_count = accessibility_data.get("unused_count", len(unused_ids))
            if unused_count > unused_threshold:
                items = "".join(
                    [f"<li><code>{uid}</code></li>" for uid in unused_ids[:10]]
                )
                more = (
                    f"<li><em>...and {unused_count - 10} more</em></li>"
                    if unused_count > 10
                    else ""
                )
                recs.append(f"""
                <div class="recommendation">
                    <h4>Remove Unused Accessibility IDs</h4>
                    <p>{unused_count} identifier(s) are defined but never used in tests:</p>
                    <ul>{items}{more}</ul>
                    <p>Consider removing these or adding test coverage.</p>
                </div>""")

        if test_plans:
            # 4. Orphaned tests
            orphaned_count = test_plans.get("orphaned_count", 0)
            if orphaned_count > 0:
                orphaned = test_plans.get("orphaned_tests", [])
                by_class: Dict[str, int] = {}
                for t in orphaned:
                    cls = t.split("/")[0] if "/" in t else "Unknown"
                    by_class[cls] = by_class.get(cls, 0) + 1
                sorted_classes = sorted(
                    by_class.items(), key=lambda x: x[1], reverse=True
                )
                items = "".join(
                    [
                        f"<li><code>{cls}</code> ({count} orphaned tests)</li>"
                        for cls, count in sorted_classes[:10]
                    ]
                )
                remaining = len(sorted_classes) - 10
                more = (
                    f"<li><em>...and {remaining} more files</em></li>"
                    if remaining > 0
                    else ""
                )
                recs.append(f"""
                <div class="recommendation critical">
                    <h4>Add Orphaned Tests to Plans</h4>
                    <p>{orphaned_count} tests are not included in any test plan.
                       Top affected files:</p>
                    <ul>{items}{more}</ul>
                    <p>See <strong>Orphaned Tests</strong> section above
                       for complete list.</p>
                </div>""")

            # 5. Tests in multiple plans
            multi_count = test_plans.get("tests_in_multiple_plans_count", 0)
            if multi_count > 0:
                multi_tests = test_plans.get("tests_in_multiple_plans", [])
                items = "".join(
                    [
                        f"<li><code>{t.get('test', '')}</code> appears in "
                        f"{t.get('plan_count', 0)} plans</li>"
                        for t in multi_tests[:5]
                    ]
                )
                remaining = multi_count - 5
                more = (
                    f"<li><em>...and {remaining} more</em></li>"
                    if remaining > 0
                    else ""
                )
                recs.append(f"""
                <div class="recommendation">
                    <h4>Review Tests in Multiple Plans</h4>
                    <p>{multi_count} tests appear in more than one test plan.
                       Verify this is intentional:</p>
                    <ul>{items}{more}</ul>
                    <p>This may be intentional (e.g., smoke tests also in
                       regression), but could indicate redundant test execution.</p>
                    <p class="json-ref">See <a href="test_plans.json">test_plans.json</a> →
                       <code>tests_in_multiple_plans</code> for complete list</p>
                </div>""")

            # 6. Skipped tests
            skipped_count = test_plans.get("skipped_tests_count", 0)
            if skipped_count > 0:
                skipped = test_plans.get("skipped_tests", [])
                items = "".join([f"<li><code>{t}</code></li>" for t in skipped[:5]])
                remaining = skipped_count - 5
                more = (
                    f"<li><em>...and {remaining} more</em></li>"
                    if remaining > 0
                    else ""
                )
                recs.append(f"""
                <div class="recommendation">
                    <h4>Review Skipped Tests</h4>
                    <p>Found {skipped_count} skipped tests.
                       Verify these tests should remain skipped:</p>
                    <ul>{items}{more}</ul>
                    <p>Skipped tests may indicate:</p>
                    <ul>
                        <li>Tests that are flaky or unreliable</li>
                        <li>Tests for features not yet implemented</li>
                        <li>Tests that need to be fixed or removed</li>
                    </ul>
                    <p class="json-ref">See <a href="test_plans.json">test_plans.json</a> →
                       <code>skipped_tests</code> for details</p>
                </div>""")

        if not recs:
            return """
            <section id="recommendations">
                <h2>Recommendations</h2>
                <p style="color: var(--success); font-weight: 500;">
                    No issues found. Your test suite looks healthy!
                </p>
            </section>"""

        return f"""
        <section id="recommendations">
            <h2>Recommendations</h2>
            {"".join(recs)}
        </section>"""


def generate_html_report(
    project_path: Path,
    test_inventory: Optional[Dict[str, Any]] = None,
    accessibility_data: Optional[Dict[str, Any]] = None,
    test_plans: Optional[Dict[str, Any]] = None,
    screen_graph: Optional[Dict[str, Any]] = None,
    output_path: Optional[Path] = None,
) -> str:
    reporter = HTMLReporter(project_path=project_path)
    report = reporter.generate_report(
        test_inventory=test_inventory,
        accessibility_data=accessibility_data,
        test_plans=test_plans,
        screen_graph=screen_graph,
    )
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(report, encoding="utf-8")
    return report
